{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqphhGfOZJOuEbsNNrRTDJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a51lb/NLP-b30/blob/main/NLP%20ASSIGNMENT%207\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrIV2Q0pr5VR",
        "outputId": "96b59ea3-51d7-4aad-ae8f-e481042d3a08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nbformat': 4,\n",
              " 'nbformat_minor': 0,\n",
              " 'metadata': {'colab': {'provenance': []},\n",
              "  'kernelspec': {'name': 'python3', 'display_name': 'Python 3'},\n",
              "  'language_info': {'name': 'python'}},\n",
              " 'cells': [{'cell_type': 'markdown',\n",
              "   'source': ['**Use a simple dataset for English-to-French translation. You can either use a small dataset like this or download a more extensive dataset such as the Tab-delimited Bilingual Sentence Pairs dataset from Tatoeba or Parallel Corpus from the European Parliament.**\\n',\n",
              "    '\\n',\n",
              "    '---\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    '**Example data (small English to French pairs)**\\n',\n",
              "    '\\n',\n",
              "    '**data = [ (\"hello\", \"bonjour\"), (\"how are you\", \"comment ça va\"), (\"I am fine\", \"je vais bien\"), (\"what is your name\", \"comment tu t\\'appelles\"), (\"my name is\", \"je m\\'appelle\"), (\"thank you\", \"merci\"), (\"goodbye\", \"au revoir\") ]  [CO4]**\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    '**(a) Data Preprocessing**\\n',\n",
              "    '\\n',\n",
              "    '**(b) Build Seq2Seq Model**\\n',\n",
              "    '\\n',\n",
              "    '**(c) Preparing the Data for Training**\\n',\n",
              "    '\\n',\n",
              "    '**(d) Train the model on the dataset**\\n',\n",
              "    '\\n',\n",
              "    '**(e) Inference Setup for Translation**\\n',\n",
              "    '\\n',\n",
              "    '**(f) Translate New Sentences**\\n',\n",
              "    '\\n',\n",
              "    '**(g) Experimenting and Improving the Model by large dataset and hyper tune parameter.**\\n',\n",
              "    '\\n'],\n",
              "   'metadata': {'id': 'hmbbcNEyEBcF'}},\n",
              "  {'cell_type': 'code',\n",
              "   'source': ['import numpy as np\\n',\n",
              "    'from tensorflow.keras.models import Model\\n',\n",
              "    'from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\\n',\n",
              "    'from tensorflow.keras.preprocessing.text import Tokenizer\\n',\n",
              "    'from tensorflow.keras.preprocessing.sequence import pad_sequences\\n',\n",
              "    '\\n',\n",
              "    '# Sample dataset\\n',\n",
              "    'data = [\\n',\n",
              "    '    (\"hello\", \"bonjour\"),\\n',\n",
              "    '    (\"how are you\", \"comment ça va\"),\\n',\n",
              "    '    (\"I am fine\", \"je vais bien\"),\\n',\n",
              "    '    (\"what is your name\", \"comment tu t\\'appelles\"),\\n',\n",
              "    '    (\"my name is\", \"je m\\'appelle\"),\\n',\n",
              "    '    (\"thank you\", \"merci\"),\\n',\n",
              "    '    (\"goodbye\", \"au revoir\")\\n',\n",
              "    ']\\n',\n",
              "    '\\n',\n",
              "    '# (a) Data Preprocessing\\n',\n",
              "    'def preprocess_data(data):\\n',\n",
              "    '    # Split English and French sentences\\n',\n",
              "    '    eng_texts = [pair[0] for pair in data]\\n',\n",
              "    \"    fra_texts = ['<start> ' + pair[1] + ' <end>' for pair in data]\\n\",\n",
              "    '\\n',\n",
              "    '    # Create tokenizers\\n',\n",
              "    '    eng_tokenizer = Tokenizer()\\n',\n",
              "    '    fra_tokenizer = Tokenizer()\\n',\n",
              "    '\\n',\n",
              "    '    # Fit tokenizers\\n',\n",
              "    '    eng_tokenizer.fit_on_texts(eng_texts)\\n',\n",
              "    '    fra_tokenizer.fit_on_texts(fra_texts)\\n',\n",
              "    '\\n',\n",
              "    '    # Convert texts to sequences\\n',\n",
              "    '    eng_sequences = eng_tokenizer.texts_to_sequences(eng_texts)\\n',\n",
              "    '    fra_sequences = fra_tokenizer.texts_to_sequences(fra_texts)\\n',\n",
              "    '\\n',\n",
              "    '    # Find maximum lengths\\n',\n",
              "    '    max_eng_len = max(len(seq) for seq in eng_sequences)\\n',\n",
              "    '    max_fra_len = max(len(seq) for seq in fra_sequences)\\n',\n",
              "    '\\n',\n",
              "    '    # Pad sequences\\n',\n",
              "    \"    eng_padded = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\\n\",\n",
              "    \"    fra_padded = pad_sequences(fra_sequences, maxlen=max_fra_len, padding='post')\\n\",\n",
              "    '\\n',\n",
              "    '    return eng_padded, fra_padded, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len\\n',\n",
              "    '\\n',\n",
              "    '# Process the data\\n',\n",
              "    'eng_data, fra_data, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len = preprocess_data(data)\\n',\n",
              "    '\\n',\n",
              "    '# Get vocabulary sizes\\n',\n",
              "    'eng_vocab_size = len(eng_tokenizer.word_index) + 1\\n',\n",
              "    'fra_vocab_size = len(fra_tokenizer.word_index) + 1\\n',\n",
              "    '\\n',\n",
              "    '# Print shapes for debugging\\n',\n",
              "    'print(\"Input shapes:\")\\n',\n",
              "    'print(f\"English data shape: {eng_data.shape}\")\\n',\n",
              "    'print(f\"French data shape: {fra_data.shape}\")\\n',\n",
              "    '\\n',\n",
              "    '# (b) Build Seq2Seq Model\\n',\n",
              "    'def build_model(input_vocab, output_vocab, input_length, output_length):\\n',\n",
              "    '    # Encoder\\n',\n",
              "    '    encoder_inputs = Input(shape=(input_length,))\\n',\n",
              "    '    encoder_embedding = Embedding(input_vocab, 50)(encoder_inputs)\\n',\n",
              "    '    encoder = LSTM(100, return_state=True)\\n',\n",
              "    '    encoder_outputs, state_h, state_c = encoder(encoder_embedding)\\n',\n",
              "    '    encoder_states = [state_h, state_c]\\n',\n",
              "    '\\n',\n",
              "    '    # Decoder\\n',\n",
              "    '    decoder_inputs = Input(shape=(output_length,))\\n',\n",
              "    '    decoder_embedding = Embedding(output_vocab, 50)(decoder_inputs)\\n',\n",
              "    '    decoder_lstm = LSTM(100, return_sequences=True)\\n',\n",
              "    '    decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)\\n',\n",
              "    \"    decoder_dense = Dense(output_vocab, activation='softmax')\\n\",\n",
              "    '    decoder_outputs = decoder_dense(decoder_outputs)\\n',\n",
              "    '\\n',\n",
              "    '    # Create model\\n',\n",
              "    '    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\\n',\n",
              "    '    return model\\n',\n",
              "    '\\n',\n",
              "    '# (c) Prepare Data for Training\\n',\n",
              "    'decoder_input_data = fra_data[:, :-1]  # Remove last token\\n',\n",
              "    'decoder_target_data = fra_data[:, 1:]  # Remove first token\\n',\n",
              "    '\\n',\n",
              "    '# Print shapes for debugging\\n',\n",
              "    'print(\"\\\\nTraining data shapes:\")\\n',\n",
              "    'print(f\"Decoder input shape: {decoder_input_data.shape}\")\\n',\n",
              "    'print(f\"Decoder target shape: {decoder_target_data.shape}\")\\n',\n",
              "    '\\n',\n",
              "    '# (d) Train the Model\\n',\n",
              "    'model = build_model(\\n',\n",
              "    '    eng_vocab_size,\\n',\n",
              "    '    fra_vocab_size,\\n',\n",
              "    '    max_eng_len,\\n',\n",
              "    '    max_fra_len - 1\\n',\n",
              "    ')\\n',\n",
              "    '\\n',\n",
              "    '# Compile model\\n',\n",
              "    'model.compile(\\n',\n",
              "    \"    optimizer='adam',\\n\",\n",
              "    \"    loss='sparse_categorical_crossentropy',\\n\",\n",
              "    \"    metrics=['accuracy']\\n\",\n",
              "    ')\\n',\n",
              "    '\\n',\n",
              "    '# Train model\\n',\n",
              "    'history = model.fit(\\n',\n",
              "    '    [eng_data, decoder_input_data],\\n',\n",
              "    '    decoder_target_data,\\n',\n",
              "    '    batch_size=2,\\n',\n",
              "    '    epochs=100,\\n',\n",
              "    '    validation_split=0.2\\n',\n",
              "    ')\\n',\n",
              "    '\\n',\n",
              "    '# (e) Inference Setup\\n',\n",
              "    'class Translator:\\n',\n",
              "    '    def __init__(self, model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len):\\n',\n",
              "    '        self.model = model\\n',\n",
              "    '        self.eng_tokenizer = eng_tokenizer\\n',\n",
              "    '        self.fra_tokenizer = fra_tokenizer\\n',\n",
              "    '        self.max_eng_len = max_eng_len\\n',\n",
              "    '        self.max_fra_len = max_fra_len\\n',\n",
              "    '        self.fra_index_word = {v: k for k, v in fra_tokenizer.word_index.items()}\\n',\n",
              "    '\\n',\n",
              "    '    def translate(self, text):\\n',\n",
              "    '        # Tokenize input text\\n',\n",
              "    '        sequence = self.eng_tokenizer.texts_to_sequences([text])\\n',\n",
              "    \"        padded = pad_sequences(sequence, maxlen=self.max_eng_len, padding='post')\\n\",\n",
              "    '\\n',\n",
              "    '        # Initialize target sequence\\n',\n",
              "    '        target_seq = np.zeros((1, self.max_fra_len - 1))\\n',\n",
              "    '\\n',\n",
              "    '        # Generate translation\\n',\n",
              "    '        prediction = self.model.predict([padded, target_seq])\\n',\n",
              "    '\\n',\n",
              "    '        # Convert prediction to text\\n',\n",
              "    '        output_sequence = np.argmax(prediction[0], axis=1)\\n',\n",
              "    '        translated_text = []\\n',\n",
              "    '\\n',\n",
              "    '        for idx in output_sequence:\\n',\n",
              "    '            if idx != 0:\\n',\n",
              "    \"                word = self.fra_index_word.get(idx, '')\\n\",\n",
              "    \"                if word and word not in ['<start>', '<end>']:\\n\",\n",
              "    '                    translated_text.append(word)\\n',\n",
              "    '\\n',\n",
              "    \"        return ' '.join(translated_text)\\n\",\n",
              "    '\\n',\n",
              "    '# (f) Translate New Sentences\\n',\n",
              "    'translator = Translator(model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len)\\n',\n",
              "    '\\n',\n",
              "    '# Test translations\\n',\n",
              "    'test_sentences = [\\n',\n",
              "    '    \"hello\",\\n',\n",
              "    '    \"thank you\",\\n',\n",
              "    '    \"your name\",\\n',\n",
              "    '    \"how are you\"\\n',\n",
              "    ']\\n',\n",
              "    '\\n',\n",
              "    'print(\"\\\\nTranslations:\")\\n',\n",
              "    'for sentence in test_sentences:\\n',\n",
              "    '    translation = translator.translate(sentence)\\n',\n",
              "    '    print(f\"English: {sentence}\")\\n',\n",
              "    '    print(f\"French: {translation}\\\\n\")'],\n",
              "   'metadata': {'colab': {'base_uri': 'https://localhost:8080/'},\n",
              "    'id': 'PMvwYmYTEFZ-',\n",
              "    'outputId': '988e0f72-c4c8-4906-861d-dc5379a1d22e'},\n",
              "   'execution_count': 1,\n",
              "   'outputs': [{'output_type': 'stream',\n",
              "     'name': 'stdout',\n",
              "     'text': ['Input shapes:\\n',\n",
              "      'English data shape: (7, 4)\\n',\n",
              "      'French data shape: (7, 5)\\n',\n",
              "      '\\n',\n",
              "      'Training data shapes:\\n',\n",
              "      'Decoder input shape: (7, 4)\\n',\n",
              "      'Decoder target shape: (7, 4)\\n',\n",
              "      'Epoch 1/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m4s\\x1b[0m 222ms/step - accuracy: 0.1781 - loss: 2.7703 - val_accuracy: 0.1250 - val_loss: 2.7523\\n',\n",
              "      'Epoch 2/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 18ms/step - accuracy: 0.2500 - loss: 2.7511 - val_accuracy: 0.2500 - val_loss: 2.7370\\n',\n",
              "      'Epoch 3/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.2500 - loss: 2.7321 - val_accuracy: 0.2500 - val_loss: 2.7147\\n',\n",
              "      'Epoch 4/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 22ms/step - accuracy: 0.2500 - loss: 2.7038 - val_accuracy: 0.2500 - val_loss: 2.6822\\n',\n",
              "      'Epoch 5/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 0.2500 - loss: 2.6697 - val_accuracy: 0.2500 - val_loss: 2.6381\\n',\n",
              "      'Epoch 6/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 18ms/step - accuracy: 0.2500 - loss: 2.6374 - val_accuracy: 0.2500 - val_loss: 2.5761\\n',\n",
              "      'Epoch 7/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 23ms/step - accuracy: 0.2500 - loss: 2.5589 - val_accuracy: 0.2500 - val_loss: 2.4734\\n',\n",
              "      'Epoch 8/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 23ms/step - accuracy: 0.2500 - loss: 2.4826 - val_accuracy: 0.2500 - val_loss: 2.3248\\n',\n",
              "      'Epoch 9/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 22ms/step - accuracy: 0.2500 - loss: 2.2995 - val_accuracy: 0.2500 - val_loss: 2.1605\\n',\n",
              "      'Epoch 10/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.2500 - loss: 2.1589 - val_accuracy: 0.2500 - val_loss: 2.1536\\n',\n",
              "      'Epoch 11/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.2500 - loss: 2.1695 - val_accuracy: 0.2500 - val_loss: 2.1748\\n',\n",
              "      'Epoch 12/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.2500 - loss: 2.0422 - val_accuracy: 0.2500 - val_loss: 2.1187\\n',\n",
              "      'Epoch 13/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.2500 - loss: 2.0129 - val_accuracy: 0.2500 - val_loss: 2.0938\\n',\n",
              "      'Epoch 14/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 0.3219 - loss: 1.9148 - val_accuracy: 0.5000 - val_loss: 2.1059\\n',\n",
              "      'Epoch 15/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.4344 - loss: 1.9494 - val_accuracy: 0.5000 - val_loss: 2.1314\\n',\n",
              "      'Epoch 16/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 0.3562 - loss: 2.0671 - val_accuracy: 0.5000 - val_loss: 2.1567\\n',\n",
              "      'Epoch 17/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 0.5156 - loss: 1.9194 - val_accuracy: 0.5000 - val_loss: 2.1717\\n',\n",
              "      'Epoch 18/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.5469 - loss: 1.8345 - val_accuracy: 0.6250 - val_loss: 2.2120\\n',\n",
              "      'Epoch 19/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 26ms/step - accuracy: 0.5156 - loss: 1.8150 - val_accuracy: 0.6250 - val_loss: 2.2781\\n',\n",
              "      'Epoch 20/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 0.4844 - loss: 1.8538 - val_accuracy: 0.6250 - val_loss: 2.3406\\n',\n",
              "      'Epoch 21/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 0.4844 - loss: 1.7311 - val_accuracy: 0.6250 - val_loss: 2.3896\\n',\n",
              "      'Epoch 22/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 18ms/step - accuracy: 0.4844 - loss: 1.7434 - val_accuracy: 0.5000 - val_loss: 2.4392\\n',\n",
              "      'Epoch 23/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.5000 - loss: 1.5845 - val_accuracy: 0.5000 - val_loss: 2.5053\\n',\n",
              "      'Epoch 24/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.5156 - loss: 1.5518 - val_accuracy: 0.5000 - val_loss: 2.6016\\n',\n",
              "      'Epoch 25/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 0.4844 - loss: 1.5211 - val_accuracy: 0.5000 - val_loss: 2.7236\\n',\n",
              "      'Epoch 26/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.5156 - loss: 1.4369 - val_accuracy: 0.5000 - val_loss: 2.8668\\n',\n",
              "      'Epoch 27/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 22ms/step - accuracy: 0.4375 - loss: 1.4898 - val_accuracy: 0.3750 - val_loss: 2.9898\\n',\n",
              "      'Epoch 28/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 24ms/step - accuracy: 0.5156 - loss: 1.2883 - val_accuracy: 0.5000 - val_loss: 3.1014\\n',\n",
              "      'Epoch 29/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.5781 - loss: 1.0952 - val_accuracy: 0.5000 - val_loss: 3.2311\\n',\n",
              "      'Epoch 30/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 24ms/step - accuracy: 0.5156 - loss: 1.1805 - val_accuracy: 0.3750 - val_loss: 3.3717\\n',\n",
              "      'Epoch 31/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 0.5000 - loss: 1.1128 - val_accuracy: 0.5000 - val_loss: 3.5108\\n',\n",
              "      'Epoch 32/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.5406 - loss: 1.0409 - val_accuracy: 0.3750 - val_loss: 3.5962\\n',\n",
              "      'Epoch 33/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.5281 - loss: 1.1311 - val_accuracy: 0.3750 - val_loss: 3.7048\\n',\n",
              "      'Epoch 34/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.7094 - loss: 0.9598 - val_accuracy: 0.5000 - val_loss: 3.8452\\n',\n",
              "      'Epoch 35/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.6719 - loss: 0.9779 - val_accuracy: 0.5000 - val_loss: 3.9010\\n',\n",
              "      'Epoch 36/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.8875 - loss: 0.7809 - val_accuracy: 0.3750 - val_loss: 3.9765\\n',\n",
              "      'Epoch 37/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 0.9281 - loss: 0.7983 - val_accuracy: 0.5000 - val_loss: 4.0720\\n',\n",
              "      'Epoch 38/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 0.9594 - loss: 0.6173 - val_accuracy: 0.5000 - val_loss: 4.1424\\n',\n",
              "      'Epoch 39/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 26ms/step - accuracy: 0.9281 - loss: 0.5946 - val_accuracy: 0.5000 - val_loss: 4.1963\\n',\n",
              "      'Epoch 40/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 24ms/step - accuracy: 0.9281 - loss: 0.5609 - val_accuracy: 0.3750 - val_loss: 4.2502\\n',\n",
              "      'Epoch 41/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 1.0000 - loss: 0.5357 - val_accuracy: 0.5000 - val_loss: 4.3377\\n',\n",
              "      'Epoch 42/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.4433 - val_accuracy: 0.5000 - val_loss: 4.3683\\n',\n",
              "      'Epoch 43/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.4010 - val_accuracy: 0.5000 - val_loss: 4.4035\\n',\n",
              "      'Epoch 44/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3414 - val_accuracy: 0.5000 - val_loss: 4.5080\\n',\n",
              "      'Epoch 45/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 0.9750 - loss: 0.3553 - val_accuracy: 0.5000 - val_loss: 4.5311\\n',\n",
              "      'Epoch 46/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3182 - val_accuracy: 0.3750 - val_loss: 4.5412\\n',\n",
              "      'Epoch 47/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.2551 - val_accuracy: 0.5000 - val_loss: 4.6365\\n',\n",
              "      'Epoch 48/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.2719 - val_accuracy: 0.5000 - val_loss: 4.6560\\n',\n",
              "      'Epoch 49/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 25ms/step - accuracy: 1.0000 - loss: 0.2372 - val_accuracy: 0.3750 - val_loss: 4.6881\\n',\n",
              "      'Epoch 50/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2445 - val_accuracy: 0.3750 - val_loss: 4.6891\\n',\n",
              "      'Epoch 51/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1692 - val_accuracy: 0.5000 - val_loss: 4.7024\\n',\n",
              "      'Epoch 52/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1815 - val_accuracy: 0.5000 - val_loss: 4.6976\\n',\n",
              "      'Epoch 53/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1655 - val_accuracy: 0.5000 - val_loss: 4.6744\\n',\n",
              "      'Epoch 54/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1515 - val_accuracy: 0.5000 - val_loss: 4.7021\\n',\n",
              "      'Epoch 55/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1487 - val_accuracy: 0.5000 - val_loss: 4.7448\\n',\n",
              "      'Epoch 56/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1500 - val_accuracy: 0.5000 - val_loss: 4.7661\\n',\n",
              "      'Epoch 57/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1342 - val_accuracy: 0.5000 - val_loss: 4.7637\\n',\n",
              "      'Epoch 58/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1151 - val_accuracy: 0.5000 - val_loss: 4.7665\\n',\n",
              "      'Epoch 59/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1146 - val_accuracy: 0.5000 - val_loss: 4.7818\\n',\n",
              "      'Epoch 60/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1138 - val_accuracy: 0.5000 - val_loss: 4.7988\\n',\n",
              "      'Epoch 61/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1038 - val_accuracy: 0.5000 - val_loss: 4.8136\\n',\n",
              "      'Epoch 62/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0931 - val_accuracy: 0.5000 - val_loss: 4.8193\\n',\n",
              "      'Epoch 63/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0942 - val_accuracy: 0.5000 - val_loss: 4.8203\\n',\n",
              "      'Epoch 64/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0908 - val_accuracy: 0.5000 - val_loss: 4.8233\\n',\n",
              "      'Epoch 65/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0835 - val_accuracy: 0.5000 - val_loss: 4.8330\\n',\n",
              "      'Epoch 66/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0834 - val_accuracy: 0.5000 - val_loss: 4.8473\\n',\n",
              "      'Epoch 67/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0784 - val_accuracy: 0.5000 - val_loss: 4.8589\\n',\n",
              "      'Epoch 68/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0798 - val_accuracy: 0.5000 - val_loss: 4.8676\\n',\n",
              "      'Epoch 69/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0720 - val_accuracy: 0.5000 - val_loss: 4.8760\\n',\n",
              "      'Epoch 70/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0697 - val_accuracy: 0.5000 - val_loss: 4.8821\\n',\n",
              "      'Epoch 71/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0674 - val_accuracy: 0.5000 - val_loss: 4.8889\\n',\n",
              "      'Epoch 72/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0643 - val_accuracy: 0.5000 - val_loss: 4.8969\\n',\n",
              "      'Epoch 73/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0598 - val_accuracy: 0.5000 - val_loss: 4.9038\\n',\n",
              "      'Epoch 74/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0591 - val_accuracy: 0.5000 - val_loss: 4.9101\\n',\n",
              "      'Epoch 75/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0548 - val_accuracy: 0.5000 - val_loss: 4.9142\\n',\n",
              "      'Epoch 76/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0519 - val_accuracy: 0.5000 - val_loss: 4.9161\\n',\n",
              "      'Epoch 77/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0507 - val_accuracy: 0.5000 - val_loss: 4.9189\\n',\n",
              "      'Epoch 78/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0494 - val_accuracy: 0.5000 - val_loss: 4.9246\\n',\n",
              "      'Epoch 79/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0472 - val_accuracy: 0.5000 - val_loss: 4.9347\\n',\n",
              "      'Epoch 80/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0472 - val_accuracy: 0.5000 - val_loss: 4.9453\\n',\n",
              "      'Epoch 81/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0464 - val_accuracy: 0.5000 - val_loss: 4.9539\\n',\n",
              "      'Epoch 82/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0449 - val_accuracy: 0.5000 - val_loss: 4.9613\\n',\n",
              "      'Epoch 83/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0414 - val_accuracy: 0.5000 - val_loss: 4.9655\\n',\n",
              "      'Epoch 84/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0419 - val_accuracy: 0.5000 - val_loss: 4.9699\\n',\n",
              "      'Epoch 85/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0411 - val_accuracy: 0.5000 - val_loss: 4.9758\\n',\n",
              "      'Epoch 86/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0394 - val_accuracy: 0.5000 - val_loss: 4.9831\\n',\n",
              "      'Epoch 87/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0365 - val_accuracy: 0.5000 - val_loss: 4.9904\\n',\n",
              "      'Epoch 88/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0364 - val_accuracy: 0.5000 - val_loss: 4.9974\\n',\n",
              "      'Epoch 89/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0360 - val_accuracy: 0.5000 - val_loss: 5.0044\\n',\n",
              "      'Epoch 90/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0333 - val_accuracy: 0.5000 - val_loss: 5.0099\\n',\n",
              "      'Epoch 91/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0323 - val_accuracy: 0.5000 - val_loss: 5.0148\\n',\n",
              "      'Epoch 92/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0326 - val_accuracy: 0.5000 - val_loss: 5.0196\\n',\n",
              "      'Epoch 93/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0307 - val_accuracy: 0.5000 - val_loss: 5.0246\\n',\n",
              "      'Epoch 94/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0306 - val_accuracy: 0.5000 - val_loss: 5.0297\\n',\n",
              "      'Epoch 95/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0300 - val_accuracy: 0.5000 - val_loss: 5.0358\\n',\n",
              "      'Epoch 96/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0295 - val_accuracy: 0.5000 - val_loss: 5.0422\\n',\n",
              "      'Epoch 97/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0285 - val_accuracy: 0.5000 - val_loss: 5.0483\\n',\n",
              "      'Epoch 98/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0275 - val_accuracy: 0.5000 - val_loss: 5.0537\\n',\n",
              "      'Epoch 99/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0262 - val_accuracy: 0.5000 - val_loss: 5.0581\\n',\n",
              "      'Epoch 100/100\\n',\n",
              "      '\\x1b[1m3/3\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0261 - val_accuracy: 0.5000 - val_loss: 5.0626\\n',\n",
              "      '\\n',\n",
              "      'Translations:\\n',\n",
              "      '\\x1b[1m1/1\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 291ms/step\\n',\n",
              "      'English: hello\\n',\n",
              "      'French: bonjour end\\n',\n",
              "      '\\n',\n",
              "      '\\x1b[1m1/1\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 28ms/step\\n',\n",
              "      'English: thank you\\n',\n",
              "      'French: comment end\\n',\n",
              "      '\\n',\n",
              "      '\\x1b[1m1/1\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step\\n',\n",
              "      'English: your name\\n',\n",
              "      'French: comment end\\n',\n",
              "      '\\n',\n",
              "      '\\x1b[1m1/1\\x1b[0m \\x1b[32m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[37m\\x1b[0m \\x1b[1m0s\\x1b[0m 20ms/step\\n',\n",
              "      'English: how are you\\n',\n",
              "      'French: comment ça va end\\n',\n",
              "      '\\n']}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"provenance\": []\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"**Use a simple dataset for English-to-French translation. You can either use a small dataset like this or download a more extensive dataset such as the Tab-delimited Bilingual Sentence Pairs dataset from Tatoeba or Parallel Corpus from the European Parliament.**\\n\",\n",
        "        \"\\n\",\n",
        "        \"---\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"**Example data (small English to French pairs)**\\n\",\n",
        "        \"\\n\",\n",
        "        \"**data = [ (\\\"hello\\\", \\\"bonjour\\\"), (\\\"how are you\\\", \\\"comment ça va\\\"), (\\\"I am fine\\\", \\\"je vais bien\\\"), (\\\"what is your name\\\", \\\"comment tu t'appelles\\\"), (\\\"my name is\\\", \\\"je m'appelle\\\"), (\\\"thank you\\\", \\\"merci\\\"), (\\\"goodbye\\\", \\\"au revoir\\\") ]  [CO4]**\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"**(a) Data Preprocessing**\\n\",\n",
        "        \"\\n\",\n",
        "        \"**(b) Build Seq2Seq Model**\\n\",\n",
        "        \"\\n\",\n",
        "        \"**(c) Preparing the Data for Training**\\n\",\n",
        "        \"\\n\",\n",
        "        \"**(d) Train the model on the dataset**\\n\",\n",
        "        \"\\n\",\n",
        "        \"**(e) Inference Setup for Translation**\\n\",\n",
        "        \"\\n\",\n",
        "        \"**(f) Translate New Sentences**\\n\",\n",
        "        \"\\n\",\n",
        "        \"**(g) Experimenting and Improving the Model by large dataset and hyper tune parameter.**\\n\",\n",
        "        \"\\n\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"hmbbcNEyEBcF\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"import numpy as np\\n\",\n",
        "        \"from tensorflow.keras.models import Model\\n\",\n",
        "        \"from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\\n\",\n",
        "        \"from tensorflow.keras.preprocessing.text import Tokenizer\\n\",\n",
        "        \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Sample dataset\\n\",\n",
        "        \"data = [\\n\",\n",
        "        \"    (\\\"hello\\\", \\\"bonjour\\\"),\\n\",\n",
        "        \"    (\\\"how are you\\\", \\\"comment ça va\\\"),\\n\",\n",
        "        \"    (\\\"I am fine\\\", \\\"je vais bien\\\"),\\n\",\n",
        "        \"    (\\\"what is your name\\\", \\\"comment tu t'appelles\\\"),\\n\",\n",
        "        \"    (\\\"my name is\\\", \\\"je m'appelle\\\"),\\n\",\n",
        "        \"    (\\\"thank you\\\", \\\"merci\\\"),\\n\",\n",
        "        \"    (\\\"goodbye\\\", \\\"au revoir\\\")\\n\",\n",
        "        \"]\\n\",\n",
        "        \"\\n\",\n",
        "        \"# (a) Data Preprocessing\\n\",\n",
        "        \"def preprocess_data(data):\\n\",\n",
        "        \"    # Split English and French sentences\\n\",\n",
        "        \"    eng_texts = [pair[0] for pair in data]\\n\",\n",
        "        \"    fra_texts = ['<start> ' + pair[1] + ' <end>' for pair in data]\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Create tokenizers\\n\",\n",
        "        \"    eng_tokenizer = Tokenizer()\\n\",\n",
        "        \"    fra_tokenizer = Tokenizer()\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Fit tokenizers\\n\",\n",
        "        \"    eng_tokenizer.fit_on_texts(eng_texts)\\n\",\n",
        "        \"    fra_tokenizer.fit_on_texts(fra_texts)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Convert texts to sequences\\n\",\n",
        "        \"    eng_sequences = eng_tokenizer.texts_to_sequences(eng_texts)\\n\",\n",
        "        \"    fra_sequences = fra_tokenizer.texts_to_sequences(fra_texts)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Find maximum lengths\\n\",\n",
        "        \"    max_eng_len = max(len(seq) for seq in eng_sequences)\\n\",\n",
        "        \"    max_fra_len = max(len(seq) for seq in fra_sequences)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Pad sequences\\n\",\n",
        "        \"    eng_padded = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\\n\",\n",
        "        \"    fra_padded = pad_sequences(fra_sequences, maxlen=max_fra_len, padding='post')\\n\",\n",
        "        \"\\n\",\n",
        "        \"    return eng_padded, fra_padded, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Process the data\\n\",\n",
        "        \"eng_data, fra_data, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len = preprocess_data(data)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Get vocabulary sizes\\n\",\n",
        "        \"eng_vocab_size = len(eng_tokenizer.word_index) + 1\\n\",\n",
        "        \"fra_vocab_size = len(fra_tokenizer.word_index) + 1\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Print shapes for debugging\\n\",\n",
        "        \"print(\\\"Input shapes:\\\")\\n\",\n",
        "        \"print(f\\\"English data shape: {eng_data.shape}\\\")\\n\",\n",
        "        \"print(f\\\"French data shape: {fra_data.shape}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# (b) Build Seq2Seq Model\\n\",\n",
        "        \"def build_model(input_vocab, output_vocab, input_length, output_length):\\n\",\n",
        "        \"    # Encoder\\n\",\n",
        "        \"    encoder_inputs = Input(shape=(input_length,))\\n\",\n",
        "        \"    encoder_embedding = Embedding(input_vocab, 50)(encoder_inputs)\\n\",\n",
        "        \"    encoder = LSTM(100, return_state=True)\\n\",\n",
        "        \"    encoder_outputs, state_h, state_c = encoder(encoder_embedding)\\n\",\n",
        "        \"    encoder_states = [state_h, state_c]\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Decoder\\n\",\n",
        "        \"    decoder_inputs = Input(shape=(output_length,))\\n\",\n",
        "        \"    decoder_embedding = Embedding(output_vocab, 50)(decoder_inputs)\\n\",\n",
        "        \"    decoder_lstm = LSTM(100, return_sequences=True)\\n\",\n",
        "        \"    decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)\\n\",\n",
        "        \"    decoder_dense = Dense(output_vocab, activation='softmax')\\n\",\n",
        "        \"    decoder_outputs = decoder_dense(decoder_outputs)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Create model\\n\",\n",
        "        \"    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\\n\",\n",
        "        \"    return model\\n\",\n",
        "        \"\\n\",\n",
        "        \"# (c) Prepare Data for Training\\n\",\n",
        "        \"decoder_input_data = fra_data[:, :-1]  # Remove last token\\n\",\n",
        "        \"decoder_target_data = fra_data[:, 1:]  # Remove first token\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Print shapes for debugging\\n\",\n",
        "        \"print(\\\"\\\\nTraining data shapes:\\\")\\n\",\n",
        "        \"print(f\\\"Decoder input shape: {decoder_input_data.shape}\\\")\\n\",\n",
        "        \"print(f\\\"Decoder target shape: {decoder_target_data.shape}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# (d) Train the Model\\n\",\n",
        "        \"model = build_model(\\n\",\n",
        "        \"    eng_vocab_size,\\n\",\n",
        "        \"    fra_vocab_size,\\n\",\n",
        "        \"    max_eng_len,\\n\",\n",
        "        \"    max_fra_len - 1\\n\",\n",
        "        \")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Compile model\\n\",\n",
        "        \"model.compile(\\n\",\n",
        "        \"    optimizer='adam',\\n\",\n",
        "        \"    loss='sparse_categorical_crossentropy',\\n\",\n",
        "        \"    metrics=['accuracy']\\n\",\n",
        "        \")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Train model\\n\",\n",
        "        \"history = model.fit(\\n\",\n",
        "        \"    [eng_data, decoder_input_data],\\n\",\n",
        "        \"    decoder_target_data,\\n\",\n",
        "        \"    batch_size=2,\\n\",\n",
        "        \"    epochs=100,\\n\",\n",
        "        \"    validation_split=0.2\\n\",\n",
        "        \")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# (e) Inference Setup\\n\",\n",
        "        \"class Translator:\\n\",\n",
        "        \"    def __init__(self, model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len):\\n\",\n",
        "        \"        self.model = model\\n\",\n",
        "        \"        self.eng_tokenizer = eng_tokenizer\\n\",\n",
        "        \"        self.fra_tokenizer = fra_tokenizer\\n\",\n",
        "        \"        self.max_eng_len = max_eng_len\\n\",\n",
        "        \"        self.max_fra_len = max_fra_len\\n\",\n",
        "        \"        self.fra_index_word = {v: k for k, v in fra_tokenizer.word_index.items()}\\n\",\n",
        "        \"\\n\",\n",
        "        \"    def translate(self, text):\\n\",\n",
        "        \"        # Tokenize input text\\n\",\n",
        "        \"        sequence = self.eng_tokenizer.texts_to_sequences([text])\\n\",\n",
        "        \"        padded = pad_sequences(sequence, maxlen=self.max_eng_len, padding='post')\\n\",\n",
        "        \"\\n\",\n",
        "        \"        # Initialize target sequence\\n\",\n",
        "        \"        target_seq = np.zeros((1, self.max_fra_len - 1))\\n\",\n",
        "        \"\\n\",\n",
        "        \"        # Generate translation\\n\",\n",
        "        \"        prediction = self.model.predict([padded, target_seq])\\n\",\n",
        "        \"\\n\",\n",
        "        \"        # Convert prediction to text\\n\",\n",
        "        \"        output_sequence = np.argmax(prediction[0], axis=1)\\n\",\n",
        "        \"        translated_text = []\\n\",\n",
        "        \"\\n\",\n",
        "        \"        for idx in output_sequence:\\n\",\n",
        "        \"            if idx != 0:\\n\",\n",
        "        \"                word = self.fra_index_word.get(idx, '')\\n\",\n",
        "        \"                if word and word not in ['<start>', '<end>']:\\n\",\n",
        "        \"                    translated_text.append(word)\\n\",\n",
        "        \"\\n\",\n",
        "        \"        return ' '.join(translated_text)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# (f) Translate New Sentences\\n\",\n",
        "        \"translator = Translator(model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Test translations\\n\",\n",
        "        \"test_sentences = [\\n\",\n",
        "        \"    \\\"hello\\\",\\n\",\n",
        "        \"    \\\"thank you\\\",\\n\",\n",
        "        \"    \\\"your name\\\",\\n\",\n",
        "        \"    \\\"how are you\\\"\\n\",\n",
        "        \"]\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nTranslations:\\\")\\n\",\n",
        "        \"for sentence in test_sentences:\\n\",\n",
        "        \"    translation = translator.translate(sentence)\\n\",\n",
        "        \"    print(f\\\"English: {sentence}\\\")\\n\",\n",
        "        \"    print(f\\\"French: {translation}\\\\n\\\")\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\"\n",
        "        },\n",
        "        \"id\": \"PMvwYmYTEFZ-\",\n",
        "        \"outputId\": \"988e0f72-c4c8-4906-861d-dc5379a1d22e\"\n",
        "      },\n",
        "      \"execution_count\": 1,\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stdout\",\n",
        "          \"text\": [\n",
        "            \"Input shapes:\\n\",\n",
        "            \"English data shape: (7, 4)\\n\",\n",
        "            \"French data shape: (7, 5)\\n\",\n",
        "            \"\\n\",\n",
        "            \"Training data shapes:\\n\",\n",
        "            \"Decoder input shape: (7, 4)\\n\",\n",
        "            \"Decoder target shape: (7, 4)\\n\",\n",
        "            \"Epoch 1/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m4s\\u001b[0m 222ms/step - accuracy: 0.1781 - loss: 2.7703 - val_accuracy: 0.1250 - val_loss: 2.7523\\n\",\n",
        "            \"Epoch 2/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 18ms/step - accuracy: 0.2500 - loss: 2.7511 - val_accuracy: 0.2500 - val_loss: 2.7370\\n\",\n",
        "            \"Epoch 3/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.2500 - loss: 2.7321 - val_accuracy: 0.2500 - val_loss: 2.7147\\n\",\n",
        "            \"Epoch 4/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 22ms/step - accuracy: 0.2500 - loss: 2.7038 - val_accuracy: 0.2500 - val_loss: 2.6822\\n\",\n",
        "            \"Epoch 5/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 0.2500 - loss: 2.6697 - val_accuracy: 0.2500 - val_loss: 2.6381\\n\",\n",
        "            \"Epoch 6/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 18ms/step - accuracy: 0.2500 - loss: 2.6374 - val_accuracy: 0.2500 - val_loss: 2.5761\\n\",\n",
        "            \"Epoch 7/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 23ms/step - accuracy: 0.2500 - loss: 2.5589 - val_accuracy: 0.2500 - val_loss: 2.4734\\n\",\n",
        "            \"Epoch 8/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 23ms/step - accuracy: 0.2500 - loss: 2.4826 - val_accuracy: 0.2500 - val_loss: 2.3248\\n\",\n",
        "            \"Epoch 9/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 22ms/step - accuracy: 0.2500 - loss: 2.2995 - val_accuracy: 0.2500 - val_loss: 2.1605\\n\",\n",
        "            \"Epoch 10/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.2500 - loss: 2.1589 - val_accuracy: 0.2500 - val_loss: 2.1536\\n\",\n",
        "            \"Epoch 11/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.2500 - loss: 2.1695 - val_accuracy: 0.2500 - val_loss: 2.1748\\n\",\n",
        "            \"Epoch 12/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.2500 - loss: 2.0422 - val_accuracy: 0.2500 - val_loss: 2.1187\\n\",\n",
        "            \"Epoch 13/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.2500 - loss: 2.0129 - val_accuracy: 0.2500 - val_loss: 2.0938\\n\",\n",
        "            \"Epoch 14/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 0.3219 - loss: 1.9148 - val_accuracy: 0.5000 - val_loss: 2.1059\\n\",\n",
        "            \"Epoch 15/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.4344 - loss: 1.9494 - val_accuracy: 0.5000 - val_loss: 2.1314\\n\",\n",
        "            \"Epoch 16/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 0.3562 - loss: 2.0671 - val_accuracy: 0.5000 - val_loss: 2.1567\\n\",\n",
        "            \"Epoch 17/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 0.5156 - loss: 1.9194 - val_accuracy: 0.5000 - val_loss: 2.1717\\n\",\n",
        "            \"Epoch 18/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.5469 - loss: 1.8345 - val_accuracy: 0.6250 - val_loss: 2.2120\\n\",\n",
        "            \"Epoch 19/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 26ms/step - accuracy: 0.5156 - loss: 1.8150 - val_accuracy: 0.6250 - val_loss: 2.2781\\n\",\n",
        "            \"Epoch 20/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 0.4844 - loss: 1.8538 - val_accuracy: 0.6250 - val_loss: 2.3406\\n\",\n",
        "            \"Epoch 21/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 0.4844 - loss: 1.7311 - val_accuracy: 0.6250 - val_loss: 2.3896\\n\",\n",
        "            \"Epoch 22/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 18ms/step - accuracy: 0.4844 - loss: 1.7434 - val_accuracy: 0.5000 - val_loss: 2.4392\\n\",\n",
        "            \"Epoch 23/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.5000 - loss: 1.5845 - val_accuracy: 0.5000 - val_loss: 2.5053\\n\",\n",
        "            \"Epoch 24/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.5156 - loss: 1.5518 - val_accuracy: 0.5000 - val_loss: 2.6016\\n\",\n",
        "            \"Epoch 25/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 0.4844 - loss: 1.5211 - val_accuracy: 0.5000 - val_loss: 2.7236\\n\",\n",
        "            \"Epoch 26/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.5156 - loss: 1.4369 - val_accuracy: 0.5000 - val_loss: 2.8668\\n\",\n",
        "            \"Epoch 27/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 22ms/step - accuracy: 0.4375 - loss: 1.4898 - val_accuracy: 0.3750 - val_loss: 2.9898\\n\",\n",
        "            \"Epoch 28/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 24ms/step - accuracy: 0.5156 - loss: 1.2883 - val_accuracy: 0.5000 - val_loss: 3.1014\\n\",\n",
        "            \"Epoch 29/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.5781 - loss: 1.0952 - val_accuracy: 0.5000 - val_loss: 3.2311\\n\",\n",
        "            \"Epoch 30/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 24ms/step - accuracy: 0.5156 - loss: 1.1805 - val_accuracy: 0.3750 - val_loss: 3.3717\\n\",\n",
        "            \"Epoch 31/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 0.5000 - loss: 1.1128 - val_accuracy: 0.5000 - val_loss: 3.5108\\n\",\n",
        "            \"Epoch 32/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.5406 - loss: 1.0409 - val_accuracy: 0.3750 - val_loss: 3.5962\\n\",\n",
        "            \"Epoch 33/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.5281 - loss: 1.1311 - val_accuracy: 0.3750 - val_loss: 3.7048\\n\",\n",
        "            \"Epoch 34/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.7094 - loss: 0.9598 - val_accuracy: 0.5000 - val_loss: 3.8452\\n\",\n",
        "            \"Epoch 35/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.6719 - loss: 0.9779 - val_accuracy: 0.5000 - val_loss: 3.9010\\n\",\n",
        "            \"Epoch 36/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.8875 - loss: 0.7809 - val_accuracy: 0.3750 - val_loss: 3.9765\\n\",\n",
        "            \"Epoch 37/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 0.9281 - loss: 0.7983 - val_accuracy: 0.5000 - val_loss: 4.0720\\n\",\n",
        "            \"Epoch 38/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 0.9594 - loss: 0.6173 - val_accuracy: 0.5000 - val_loss: 4.1424\\n\",\n",
        "            \"Epoch 39/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 26ms/step - accuracy: 0.9281 - loss: 0.5946 - val_accuracy: 0.5000 - val_loss: 4.1963\\n\",\n",
        "            \"Epoch 40/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 24ms/step - accuracy: 0.9281 - loss: 0.5609 - val_accuracy: 0.3750 - val_loss: 4.2502\\n\",\n",
        "            \"Epoch 41/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.5357 - val_accuracy: 0.5000 - val_loss: 4.3377\\n\",\n",
        "            \"Epoch 42/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.4433 - val_accuracy: 0.5000 - val_loss: 4.3683\\n\",\n",
        "            \"Epoch 43/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.4010 - val_accuracy: 0.5000 - val_loss: 4.4035\\n\",\n",
        "            \"Epoch 44/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3414 - val_accuracy: 0.5000 - val_loss: 4.5080\\n\",\n",
        "            \"Epoch 45/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 0.9750 - loss: 0.3553 - val_accuracy: 0.5000 - val_loss: 4.5311\\n\",\n",
        "            \"Epoch 46/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3182 - val_accuracy: 0.3750 - val_loss: 4.5412\\n\",\n",
        "            \"Epoch 47/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.2551 - val_accuracy: 0.5000 - val_loss: 4.6365\\n\",\n",
        "            \"Epoch 48/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.2719 - val_accuracy: 0.5000 - val_loss: 4.6560\\n\",\n",
        "            \"Epoch 49/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.2372 - val_accuracy: 0.3750 - val_loss: 4.6881\\n\",\n",
        "            \"Epoch 50/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2445 - val_accuracy: 0.3750 - val_loss: 4.6891\\n\",\n",
        "            \"Epoch 51/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1692 - val_accuracy: 0.5000 - val_loss: 4.7024\\n\",\n",
        "            \"Epoch 52/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1815 - val_accuracy: 0.5000 - val_loss: 4.6976\\n\",\n",
        "            \"Epoch 53/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1655 - val_accuracy: 0.5000 - val_loss: 4.6744\\n\",\n",
        "            \"Epoch 54/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1515 - val_accuracy: 0.5000 - val_loss: 4.7021\\n\",\n",
        "            \"Epoch 55/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1487 - val_accuracy: 0.5000 - val_loss: 4.7448\\n\",\n",
        "            \"Epoch 56/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1500 - val_accuracy: 0.5000 - val_loss: 4.7661\\n\",\n",
        "            \"Epoch 57/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1342 - val_accuracy: 0.5000 - val_loss: 4.7637\\n\",\n",
        "            \"Epoch 58/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1151 - val_accuracy: 0.5000 - val_loss: 4.7665\\n\",\n",
        "            \"Epoch 59/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1146 - val_accuracy: 0.5000 - val_loss: 4.7818\\n\",\n",
        "            \"Epoch 60/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1138 - val_accuracy: 0.5000 - val_loss: 4.7988\\n\",\n",
        "            \"Epoch 61/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1038 - val_accuracy: 0.5000 - val_loss: 4.8136\\n\",\n",
        "            \"Epoch 62/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0931 - val_accuracy: 0.5000 - val_loss: 4.8193\\n\",\n",
        "            \"Epoch 63/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0942 - val_accuracy: 0.5000 - val_loss: 4.8203\\n\",\n",
        "            \"Epoch 64/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0908 - val_accuracy: 0.5000 - val_loss: 4.8233\\n\",\n",
        "            \"Epoch 65/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0835 - val_accuracy: 0.5000 - val_loss: 4.8330\\n\",\n",
        "            \"Epoch 66/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0834 - val_accuracy: 0.5000 - val_loss: 4.8473\\n\",\n",
        "            \"Epoch 67/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0784 - val_accuracy: 0.5000 - val_loss: 4.8589\\n\",\n",
        "            \"Epoch 68/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0798 - val_accuracy: 0.5000 - val_loss: 4.8676\\n\",\n",
        "            \"Epoch 69/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0720 - val_accuracy: 0.5000 - val_loss: 4.8760\\n\",\n",
        "            \"Epoch 70/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0697 - val_accuracy: 0.5000 - val_loss: 4.8821\\n\",\n",
        "            \"Epoch 71/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0674 - val_accuracy: 0.5000 - val_loss: 4.8889\\n\",\n",
        "            \"Epoch 72/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0643 - val_accuracy: 0.5000 - val_loss: 4.8969\\n\",\n",
        "            \"Epoch 73/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0598 - val_accuracy: 0.5000 - val_loss: 4.9038\\n\",\n",
        "            \"Epoch 74/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0591 - val_accuracy: 0.5000 - val_loss: 4.9101\\n\",\n",
        "            \"Epoch 75/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0548 - val_accuracy: 0.5000 - val_loss: 4.9142\\n\",\n",
        "            \"Epoch 76/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0519 - val_accuracy: 0.5000 - val_loss: 4.9161\\n\",\n",
        "            \"Epoch 77/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0507 - val_accuracy: 0.5000 - val_loss: 4.9189\\n\",\n",
        "            \"Epoch 78/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0494 - val_accuracy: 0.5000 - val_loss: 4.9246\\n\",\n",
        "            \"Epoch 79/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0472 - val_accuracy: 0.5000 - val_loss: 4.9347\\n\",\n",
        "            \"Epoch 80/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0472 - val_accuracy: 0.5000 - val_loss: 4.9453\\n\",\n",
        "            \"Epoch 81/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0464 - val_accuracy: 0.5000 - val_loss: 4.9539\\n\",\n",
        "            \"Epoch 82/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0449 - val_accuracy: 0.5000 - val_loss: 4.9613\\n\",\n",
        "            \"Epoch 83/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0414 - val_accuracy: 0.5000 - val_loss: 4.9655\\n\",\n",
        "            \"Epoch 84/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0419 - val_accuracy: 0.5000 - val_loss: 4.9699\\n\",\n",
        "            \"Epoch 85/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0411 - val_accuracy: 0.5000 - val_loss: 4.9758\\n\",\n",
        "            \"Epoch 86/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0394 - val_accuracy: 0.5000 - val_loss: 4.9831\\n\",\n",
        "            \"Epoch 87/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0365 - val_accuracy: 0.5000 - val_loss: 4.9904\\n\",\n",
        "            \"Epoch 88/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0364 - val_accuracy: 0.5000 - val_loss: 4.9974\\n\",\n",
        "            \"Epoch 89/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0360 - val_accuracy: 0.5000 - val_loss: 5.0044\\n\",\n",
        "            \"Epoch 90/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0333 - val_accuracy: 0.5000 - val_loss: 5.0099\\n\",\n",
        "            \"Epoch 91/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0323 - val_accuracy: 0.5000 - val_loss: 5.0148\\n\",\n",
        "            \"Epoch 92/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0326 - val_accuracy: 0.5000 - val_loss: 5.0196\\n\",\n",
        "            \"Epoch 93/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0307 - val_accuracy: 0.5000 - val_loss: 5.0246\\n\",\n",
        "            \"Epoch 94/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0306 - val_accuracy: 0.5000 - val_loss: 5.0297\\n\",\n",
        "            \"Epoch 95/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0300 - val_accuracy: 0.5000 - val_loss: 5.0358\\n\",\n",
        "            \"Epoch 96/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0295 - val_accuracy: 0.5000 - val_loss: 5.0422\\n\",\n",
        "            \"Epoch 97/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0285 - val_accuracy: 0.5000 - val_loss: 5.0483\\n\",\n",
        "            \"Epoch 98/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0275 - val_accuracy: 0.5000 - val_loss: 5.0537\\n\",\n",
        "            \"Epoch 99/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0262 - val_accuracy: 0.5000 - val_loss: 5.0581\\n\",\n",
        "            \"Epoch 100/100\\n\",\n",
        "            \"\\u001b[1m3/3\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0261 - val_accuracy: 0.5000 - val_loss: 5.0626\\n\",\n",
        "            \"\\n\",\n",
        "            \"Translations:\\n\",\n",
        "            \"\\u001b[1m1/1\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 291ms/step\\n\",\n",
        "            \"English: hello\\n\",\n",
        "            \"French: bonjour end\\n\",\n",
        "            \"\\n\",\n",
        "            \"\\u001b[1m1/1\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 28ms/step\\n\",\n",
        "            \"English: thank you\\n\",\n",
        "            \"French: comment end\\n\",\n",
        "            \"\\n\",\n",
        "            \"\\u001b[1m1/1\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step\\n\",\n",
        "            \"English: your name\\n\",\n",
        "            \"French: comment end\\n\",\n",
        "            \"\\n\",\n",
        "            \"\\u001b[1m1/1\\u001b[0m \\u001b[32m━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[37m\\u001b[0m \\u001b[1m0s\\u001b[0m 20ms/step\\n\",\n",
        "            \"English: how are you\\n\",\n",
        "            \"French: comment ça va end\\n\",\n",
        "            \"\\n\"\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ]
    }
  ]
}